[
["index.html", "Tutorial for STA2002 Prerequisites", " Tutorial for STA2002 Kun HUANG(SDS) 2020-09-29 Prerequisites Probability and Statistics I(STA2001) is the prerequisite, which mainly includes the following contents, Some usual distributions, like Binomial, Poisson, Normal, Exponential, Gamma, and Chi-square distributions (Relationships among some univariate distributions(Song 2005)); Basic terminologies, e.g.,independence, expectation, variation, correlation (coefficient), Bayes, and etc; Large number theorem, like Central Limit Theorem(CLT). References "],
["sec-T1.html", "Chapter 1 Tutorial 1 1.1 Q1 1.2 Q2 1.3 Q3", " Chapter 1 Tutorial 1 1.1 Q1 Moment-generating function \\(M(t)\\) of a random variable \\(X\\) defined in \\(D\\) that has a density function \\(f(x)\\). \\[\\begin{align} M(t) = \\mathbb{E}(e^{tx}) &amp;= \\int_{D} e^{tx}f(x)dx\\\\ \\mathbb{E}(X^{s}) &amp;= M^{(s)}(0) \\end{align}\\] Relationship between \\(\\bar X=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\) and \\(S^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(X_i-\\bar X)^2\\), independent. How to derive a quantity following \\(t\\) distribution from a norm population. \\[\\begin{align} T=\\frac{\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) S^{2}}{\\sigma^{2}} /(n-1)}}=\\frac{\\bar{X}-\\mu}{S / \\sqrt{n}} \\end{align}\\] The \\(t\\) distribution is symmetric, i.e., \\(t_{q}(n) = -t_{1-q}(n), q\\in(0,1)\\). For example, qt(0.025, 8, lower.tail = F) ## [1] 2.306004 -qt(1 - 0.025, 8, lower.tail = F) ## [1] 2.306004 Properties of \\(F\\) distribution: \\(F_{0.95}(9,24)= \\frac{1}{F_{0.05}(24,9)}\\) 1.2 Q2 Standardize a norm distribution \\(X\\in\\mathcal{N}(\\mu,\\sigma^2)\\), i.e., \\(\\frac{X-\\mu}{\\sigma}\\in\\mathcal{N}(0,1)\\). The distribution of \\(\\bar X\\) and \\(S^2\\). 1.3 Q3 Central Limit Theorem(CLT) Theorem 1.1 (Central Limit Theorem) Let \\(X_1,\\ldots,X_n\\) be independent, identically distributed (i.i.d.) random variables with finite expectation \\(\\mu\\), and positive, finite variance \\(\\sigma^2\\), and set \\(S_n=X_1 + X_2 + \\cdots + X_n\\), \\(n \\ge 1\\). Then \\[ \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}\\xrightarrow{L} N(0, 1) ~\\mathrm{as}~n\\rightarrow\\infty. \\] The relationship between Binomial distribution \\(b(n,p)\\) and Poisson distribution \\(\\mathrm{Pois}(\\lambda)\\): \\(\\infty &gt; np = \\lambda, n\\rightarrow\\infty\\) Aware the power of CLT. "],
["sec-T2.html", "Chapter 2 Tutorial 2 2.1 Q1 2.2 Q2 2.3 Q3 2.4 Q4", " Chapter 2 Tutorial 2 2.1 Q1 Derive moments from a given pdf \\(f(x)\\). \\(EX = \\int xf(x)dx, EX^2=\\int x^2f(x)dx\\). Derive variance from the first and second moments,i.e., \\(Var(X)=EX^2-E^2X\\). \\(E(aX+bY+c) = aEX + bEY+c\\), \\(Var(aX+bY+c) = a^2Var(X)+b^2Var(Y)\\). The latter needs \\(X\\) and \\(Y\\) are independent. CLT approximation. 2.2 Q2 Definition 2.1 (Poisson Process) Let \\(N(t)\\) be the number of events happens during the time interval \\([0,t]\\), if \\(N(t)\\) satisfies the following: N(0) = 0; has independent increments, and \\(\\forall \\tau&gt;0\\), \\(P(N(t+\\tau)-N(t) = n)= \\frac{(\\lambda \\tau)^n}{n!}e^{-\\lambda \\tau}\\) we call \\(\\{N(t),t\\geq0\\}\\) is a Poisson process with rate \\(\\lambda\\). Let \\((W_n&gt;t)\\) be the \\(n-th\\) random event happens after time \\(t\\), then \\(W_n\\sim Gamma(n, \\lambda)\\). In fact, \\(Gamma(n,\\lambda)\\) can be seen as the time to be waited until the \\(n-th\\) event. 2.3 Q3 Proposition 2.1 Suppose the random variable \\(X\\) has a pdf \\(f(x)\\), let \\(Y = T(X)\\), where \\(T:\\mathbb{R}\\rightarrow \\mathbb{R}\\) is an invertible transformation. Then the pdf \\(g(y)\\) of \\(Y\\) is \\[\\begin{equation*} g(y) = f(T^{-1}(y))\\frac{d T^{-1}(y)}{dy} \\end{equation*}\\] For example, suppose \\(X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\), then \\(Y=aX+b\\sim\\mathcal{N}(a\\mu+b, (a\\sigma)^2)\\). 2.4 Q4 Theorem 2.1 (Chebyshev’s Inequality) Let \\(X\\) be a random variable with finite mean \\(\\mu\\) and variance \\(\\sigma^2&gt;0\\). Then \\(\\forall k&gt;0\\), \\[\\begin{equation*} P(|X-\\mu|\\geq k\\sigma) \\leq \\frac{1}{k^2} \\end{equation*}\\] Additionally, let \\(k\\sigma = \\varepsilon\\), the above becomes, \\[\\begin{equation*} P(|X-\\mu|\\geq \\varepsilon) \\leq \\frac{\\sigma^2}{\\varepsilon^2} \\end{equation*}\\] "],
["tutorial-3.html", "Chapter 3 Tutorial 3 3.1 Method of moment estimator(MME) 3.2 Maximum likelihood estimator(MLE).", " Chapter 3 Tutorial 3 3.1 Method of moment estimator(MME) Suppose that the problem is to estimate \\(k\\) unknown parameters \\(\\boldsymbol{\\theta} := (\\theta_1,\\theta_2,...,\\theta_k)^{T}\\) characterizing the distribution \\(f_X(x;\\boldsymbol{\\theta})\\) of the random variable \\(X\\). Suppose the first \\(k\\) moments of the true distribution can be expressed by the function of \\(\\boldsymbol{\\theta}\\), i.e., \\[\\begin{align} \\mu_{1} &amp; \\equiv \\mathrm{E}[W]=g_{1}\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k}\\right) \\\\ \\mu_{2} &amp; \\equiv \\mathrm{E}\\left[W^{2}\\right]=g_{2}\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k}\\right) \\\\ &amp; \\vdots \\\\ \\mu_{k} &amp; \\equiv \\mathrm{E}\\left[W^{k}\\right]=g_{k}\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k}\\right) \\end{align}\\] Suppose a sample of size \\(n\\) is drawn, having the values of \\(x_1,x_2,...,x_n\\), let \\[\\begin{align} \\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_i^j, j=1,2,...,k \\end{align}\\] Solve the above \\(k\\) equations, we derive the method of moment estimator of \\(\\boldsymbol{\\theta}\\). 3.2 Maximum likelihood estimator(MLE). Suppose we have a sample of size \\(n\\), \\(X_1,...,X_n\\) i.i.d drawn from a population distribution \\(f_X(x;\\boldsymbol{\\theta}), \\boldsymbol{\\theta} = (\\theta_1,\\theta_2,...,\\theta_k)^{T}\\). Define the likelihood function to be \\[ L(\\boldsymbol\\theta) = \\prod_{i=1}^n f(x_i;\\boldsymbol{\\theta}) \\] The log-likelihood function is defined by \\(\\ell(\\boldsymbol\\theta)=\\log L(\\boldsymbol\\theta)\\). The maximum likelihood estimator \\(\\hat{\\boldsymbol\\theta}\\) is determined to maximize \\(L(\\boldsymbol\\theta)\\), i.e., \\[\\begin{equation} \\hat{\\boldsymbol\\theta} =\\underset{\\boldsymbol\\theta\\in \\Theta}{\\max} L(\\boldsymbol\\theta) \\end{equation}\\] "],
["confidence-interval.html", "Chapter 4 Confidence Interval 4.1 Q1 4.2 Q2 4.3 Q3 4.4 Q4 4.5 Solutions", " Chapter 4 Confidence Interval Definition 4.1 (Confidence Interval) Given a sample \\(X_1,X_2,...,X_n\\) of the population \\(X\\sim f(x;\\theta)\\) and \\(\\alpha\\in[0,1]\\), a \\((1-\\alpha)\\) confidence interval \\(\\left(a(X_1,X_2,...,X_n), b(X_1,X_2,...,X_n)\\right)\\) for the parameter \\(\\theta\\) is defined such that, \\[\\begin{equation} P\\left[a(X_1,X_2,...,X_n)&lt; \\theta &lt;b(X_1,X_2,...,X_n)\\right] = 1-\\alpha \\end{equation}\\] Interpretation and misunderstanding 4.1 Q1 Definition 4.2 (t-distribution) Suppose \\(X\\sim N(0, 1)\\), \\(U\\sim \\chi^2(n)\\), and \\(X\\) are independent from \\(Y\\), then \\(\\frac{X}{\\sqrt{U/n}}\\) has a (student) t distribution with n degrees of freedom, i.e., \\[ \\frac{X}{\\sqrt{U/n}} \\sim t(n) \\] Confidence Interval of normal population \\(X_i\\stackrel{i.i.d.}\\sim \\mathcal{N}(\\mu,\\sigma^2),i=1,2,...,n\\). We have \\[\\begin{align} \\bar X \\sim \\mathcal{N}(\\mu,\\frac{\\sigma^2}{n}), \\quad \\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(n-1) \\end{align}\\] It can be proved that \\(\\bar X\\) and \\(S^2\\) are independent. Then, \\[\\begin{equation} \\frac{\\frac{\\bar X - \\mu}{\\sqrt{\\sigma^2/n}}}{\\sqrt{\\frac{(n-1)S^2}{\\sigma^2} / (n-1)}} = \\frac{\\bar X -\\mu}{S/\\sqrt{n}}\\sim t(n-1) \\tag{4.1} \\end{equation}\\] We call such a method the pivotal approach. A pivotal quantity or pivot is a function of observations and unobservable parameters such that the function’s probability distribution does not depend on the unknown parameters. For example, \\(\\frac{\\bar X - \\mu}{\\sqrt{\\sigma^2/n}}\\sim \\mathcal{N}(0,1)\\) is a pivot. From (4.1), we derive the \\((1-\\alpha)\\) confidence interval for the mean \\(\\mu\\) when \\(\\sigma^2\\) is unknown, i.e., \\[\\begin{equation} \\bar X \\pm t_{\\alpha/2}(n-1)\\frac{S}{\\sqrt{n}} \\end{equation}\\] qt(0.05/2, 8, lower.tail = F) ## [1] 2.306004 qnorm(0.01/2, lower.tail = F) ## [1] 2.575829 4.2 Q2 Theorem 4.1 (Welch’s t-interval) Let \\(X_1,X_2,...,X_n\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(\\mu_X,\\sigma^2_X)\\) and \\(Y_1,Y_2,...,Y_m\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(\\mu_Y,\\sigma^2_Y)\\) be independent random variables. Then an approximate \\((1-\\alpha)\\) C.I. for \\(\\mu_X-\\mu_Y\\) is \\[ \\bar{X}-\\bar{Y} \\pm t_{\\alpha / 2}(r) \\sqrt{\\frac{S_{X}^{2}}{n}+\\frac{S_{Y}^{2}}{m}} \\] where \\[ r=\\left\\lfloor\\frac{\\left(\\frac{S_{\\mathrm{X}}^{2}}{n}+\\frac{S_{\\mathrm{X}}^{2}}{m}\\right)^{2}}{\\frac{1}{n-1}\\left(\\frac{S_{\\mathrm{X}}^{2}}{n}\\right)^{2}+\\frac{1}{m-1}\\left(\\frac{S_{\\mathrm{Y}}^{2}}{m}\\right)^{2}}\\right\\rfloor \\] Let \\(X_1,X_2,...,X_n\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(\\mu_X,\\sigma^2_X)\\) and \\(Y_1,Y_2,...,Y_m\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(\\mu_Y,\\sigma^2_Y)\\) be independent random variables. We have the following, \\[\\begin{align} \\bar X&amp;\\sim \\mathcal{N}(\\mu_X, \\frac{\\sigma_X^2}{n}), \\quad \\bar Y \\sim \\mathcal{N}(\\mu_Y, \\frac{\\sigma_Y^2}{n})\\\\ \\frac{(n-1)S^2_X}{\\sigma_X^2} &amp;\\sim \\chi^2(n-1), \\quad \\frac{(m-1)S^2_Y}{\\sigma_Y^2} \\sim \\chi^2(m-1) \\end{align}\\] The two samples are independent, hence, \\[\\begin{equation} \\bar X - \\bar Y \\sim \\mathcal{N}(\\mu_X-\\mu_Y,\\frac{\\sigma_X^2}{n}+\\frac{\\sigma_Y^2}{m}) \\end{equation}\\] \\(\\sigma_X=\\sigma_Y=\\sigma\\) and \\(\\sigma\\) is known, then, \\[\\begin{equation} \\frac{\\bar X - \\bar Y - (\\mu_X-\\mu_Y)}{\\sqrt{\\frac{\\sigma^2}{n}+\\frac{\\sigma^2}{m}}}\\sim \\mathcal{N}(0,1) \\end{equation}\\] \\(\\sigma_X=\\sigma_Y=\\sigma\\) and \\(\\sigma\\) is unknown, then, \\[\\begin{align} \\frac{(n-1)S_X^2+(m-1)S_Y^2}{\\sigma^2}&amp;\\sim \\chi^2(n+m-2)\\\\ \\frac{\\bar X - \\bar Y-(\\mu_X-\\mu_Y)/\\left(\\sqrt{\\frac{\\sigma^2}{n}+\\frac{\\sigma^2}{m}}\\right)}{\\sqrt{\\frac{(n-1)S_X^2+(m-1)S_Y^2}{\\sigma^2(n+m-2)}}}&amp;\\sim t(n+m-2) \\end{align}\\] \\(\\sigma_X\\neq\\sigma_Y\\) and they are both unknown, use Welch’s t-interval or CLT approximation. \\(m=n\\), then \\(Z_i = X_i-Y_i\\sim\\mathcal{N}(\\mu_X-\\mu_y,\\sigma_Z)\\) since \\((X_i,Y_i)^T\\sim \\mathcal{N}\\left((\\mu_X,\\mu_Y)^T, \\Sigma\\right)\\). Then the same technique in Q1 can be used. qt(0.05 / 2, 8, lower.tail = F) ## [1] 2.306004 4.3 Q3 If \\(X\\sim \\chi^2(n)\\) and \\(Y\\sim\\chi^2(m)\\) are independent, then \\[ \\frac{X/n}{Y/m}\\sim F(n, m) \\] Therefore, with samples from two independent normal population, i.e., let \\(X_1,X_2,...,X_n\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(\\mu_X,\\sigma^2_X)\\) and \\(Y_1,Y_2,...,Y_m\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(\\mu_Y,\\sigma^2_Y)\\) be independent, we have a pivot \\[\\begin{equation} \\frac{\\frac{(n-1)S_X^2}{\\sigma^2_X} / (n-1)}{\\frac{(m-1)S_Y^2}{\\sigma^2_Y}/(m-1)} = \\frac{S_X^2/\\sigma^2_X}{S_Y^2/\\sigma^2_Y}\\sim F(n-1, m-1) \\end{equation}\\] alpha &lt;- 0.02 qf(alpha / 2, 12, 8, lower.tail = F) ## [1] 5.666719 qf(alpha / 2, 8, 12, lower.tail = F) ## [1] 4.499365 \\[\\begin{equation} F_{1-\\alpha / 2}(r_1, r_2) = \\frac{1}{F_{\\alpha / 2}(r_2, r_1)} \\end{equation}\\] 4.4 Q4 According to the central limit theorem(CLT), we have an approximate pivot \\[\\begin{equation} \\frac{\\bar X - EX}{\\sqrt{Var X}}\\rightarrow \\mathcal{N}(0,1) \\end{equation}\\] qnorm(0.05 / 2, lower.tail = F) ## [1] 1.959964 4.5 Solutions 4.5.1 Q1 x &lt;- c(21.5, 18.95, 18.55, 19.4, 19.15, 22.35, 22.9, 22.2, 23.1) t.test(x, conf.level = 0.95) ## ## One Sample t-test ## ## data: x ## t = 33.738, df = 8, p-value = 6.506e-10 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 19.47149 22.32851 ## sample estimates: ## mean of x ## 20.9 n &lt;- qnorm(0.1/2, lower.tail = F)^2 * var(x) / (0.5)^2 print(n) ## [1] 37.37708 4.5.2 Q2 x &lt;- c(1612, 1352, 1456, 1222, 1560, 1456, 1924) y &lt;- c(1082, 1300, 1092, 1040, 910, 1248, 1092, 1040, 1092, 1288) t.test(x,y, var.equal = FALSE, conf.level = 0.95) ## ## Welch Two Sample t-test ## ## data: x and y ## t = 4.235, df = 8.5995, p-value = 0.002427 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 181.7191 604.9095 ## sample estimates: ## mean of x mean of y ## 1511.714 1118.400 Note that R use \\(t_{\\alpha/2}(8.6)\\), so the result of C.I. is different from what we use where the df=8 in t distribution. The pdf of t distribution is \\[\\begin{equation} f(t)=\\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)}\\left(1+\\frac{t^{2}}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} \\end{equation}\\] where \\(\\nu\\) is the degree of freedom. 4.5.3 Q3 r1 &lt;- 9 - 1 r2 &lt;- 13 - 1 sx &lt;- 128.41 / 12 sy &lt;- 36.72 / 8 alpha &lt;- 0.02 ci2 &lt;- sx / sy * c(qf(1 - alpha / 2, r1, r2, lower.tail = F), qf(alpha / 2, r1, r2, lower.tail = F)) ci &lt;- sqrt(ci2) print(ci2) ## [1] 0.4114085 10.4895333 print(ci) ## [1] 0.6414113 3.2387549 4.5.4 Q4 \\[ \\hat{p}_{1} \\pm z_{0.05 / 2} \\sqrt{\\frac{\\hat{p}_{1}\\left(1-\\hat{p}_{1}\\right)}{n_{1}}} \\] n1 &lt;- 194 n2 &lt;- 162 y1 &lt;- 28 y2 &lt;- 11 p1 &lt;- y1 / n1 s1 &lt;- sqrt(n1 * p1 * (1 - p1)) / n1 p1 + c(-1, 1) * qnorm(0.05/2, lower.tail = F) * s1 ## [1] 0.0948785 0.1937813 \\[ z_{\\alpha / 2} \\sqrt{\\frac{\\hat{p}_{1}\\left(1-\\hat{p}_{1}\\right)}{n}} = \\varepsilon \\] alpha &lt;- 0.1 ep &lt;- 0.04 qnorm(alpha / 2, lower.tail = F)^2 * p1 * (1 - p1) / ep^2 ## [1] 208.8321 \\[ \\left(\\hat{p}_{1}-\\hat{p}_{2}\\right)-z_{0.05} \\sqrt{\\frac{\\hat{p}_{1}\\left(1-\\hat{\\rho}_{1}\\right)}{n_{1}}+\\frac{\\hat{\\rho}_{2}\\left(1-\\hat{p}_{2}\\right)}{n_{2}}} \\] p2 &lt;- y2 / n2 p1 - p2 - qnorm(0.05, lower.tail = F) * sqrt(p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2) ## [1] 0.02370925 "],
["references.html", "References", " References Song, Wheyming Tina. 2005. “Relationships Among Some Univariate Distributions.” IIE Transactions 37 (7): 651–56. "]
]
