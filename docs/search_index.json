[
["index.html", "Tutorial for STA2002 Prerequisites", " Tutorial for STA2002 Kun HUANG(SDS) 2020-09-19 Prerequisites Probability and Statistics I(STA2001) is the prerequisite, which mainly includes the following contents, Some usual distributions, like Binomial, Poisson, Normal, Exponential, Gamma, and Chi-square distributions (Relationships among some univariate distributions(Song 2005)); Basic terminologies, e.g.,independence, expectation, variation, correlation (coefficient), Bayes, and etc; Large number theorem, like Central Limit Theorem(CLT). References "],
["sec-T1.html", "Chapter 1 Tutorial 1 1.1 Q1 1.2 Q2 1.3 Q3", " Chapter 1 Tutorial 1 1.1 Q1 Moment-generating function \\(M(t)\\) of a random variable \\(X\\) defined in \\(D\\) that has a density function \\(f(x)\\). \\[\\begin{align} M(t) = \\mathbb{E}(e^{tx}) &amp;= \\int_{D} e^{tx}f(x)dx\\\\ \\mathbb{E}(X^{s}) &amp;= M^{(s)}(0) \\end{align}\\] Relationship between \\(\\bar X=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\) and \\(S^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(X_i-\\bar X)^2\\), independent. How to derive a quantity following \\(t\\) distribution from a norm population. \\[\\begin{align} T=\\frac{\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) S^{2}}{\\sigma^{2}} /(n-1)}}=\\frac{\\bar{X}-\\mu}{S / \\sqrt{n}} \\end{align}\\] The \\(t\\) distribution is symmetric, i.e., \\(t_{q}(n) = -t_{1-q}(n), q\\in(0,1)\\). For example, qt(0.025, 8, lower.tail = F) ## [1] 2.306004 -qt(1 - 0.025, 8, lower.tail = F) ## [1] 2.306004 Properties of \\(F\\) distribution: \\(F_{0.95}(9,24)= \\frac{1}{F_{0.05}(24,9)}\\) 1.2 Q2 Standardize a norm distribution \\(X\\in\\mathcal{N}(\\mu,\\sigma^2)\\), i.e., \\(\\frac{X-\\mu}{\\sigma}\\in\\mathcal{N}(0,1)\\). The distribution of \\(\\bar X\\) and \\(S^2\\). 1.3 Q3 Central Limit Theorem(CLT) Theorem 1.1 (Central Limit Theorem) Let \\(X_1,\\ldots,X_n\\) be independent, identically distributed (i.i.d.) random variables with finite expectation \\(\\mu\\), and positive, finite variance \\(\\sigma^2\\), and set \\(S_n=X_1 + X_2 + \\cdots + X_n\\), \\(n \\ge 1\\). Then \\[ \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}\\xrightarrow{L} N(0, 1) ~\\mathrm{as}~n\\rightarrow\\infty. \\] The relationship between Binomial distribution \\(b(n,p)\\) and Poisson distribution \\(\\mathrm{Pois}(\\lambda)\\): \\(\\infty &gt; np = \\lambda, n\\rightarrow\\infty\\) Aware the power of CLT. "],
["sec-T2.html", "Chapter 2 Tutorial 2 2.1 Q1 2.2 Q2 2.3 Q3 2.4 Q4", " Chapter 2 Tutorial 2 2.1 Q1 Derive moments from a given pdf \\(f(x)\\). \\(EX = \\int xf(x)dx, EX^2=\\int x^2f(x)dx\\). Derive variance from the first and second moments,i.e., \\(Var(X)=EX^2-E^2X\\). \\(E(aX+bY+c) = aEX + bEY+c\\), \\(Var(aX+bY+c) = a^2Var(X)+b^2Var(Y)\\). The latter needs \\(X\\) and \\(Y\\) are independent. CLT approximation. 2.2 Q2 Definition 2.1 (Poisson Process) Let \\(N(t)\\) be the number of events happens during the time interval \\([0,t]\\), if \\(N(t)\\) satisfies the following: N(0) = 0; has independent increments, and \\(\\forall \\tau&gt;0\\), \\(P(N(t+\\tau)-N(t) = n)= \\frac{(\\lambda \\tau)^n}{n!}e^{-\\lambda \\tau}\\) we call \\(\\{N(t),t\\geq0\\}\\) is a Poisson process with rate \\(\\lambda\\). Let \\((W_n&gt;t)\\) be the \\(n-th\\) random event happens after time \\(t\\), then \\(W_n\\sim Gamma(n, \\lambda)\\). In fact, \\(Gamma(n,\\lambda)\\) can be seen as the time to be waited until the \\(n-th\\) event. 2.3 Q3 Proposition 2.1 Suppose the random variable \\(X\\) has a pdf \\(f(x)\\), let \\(Y = T(X)\\), where \\(T:\\mathbb{R}\\rightarrow \\mathbb{R}\\) is an invertible transformation. Then the pdf \\(g(y)\\) of \\(Y\\) is \\[\\begin{equation*} g(y) = f(T^{-1}(y))\\frac{d T^{-1}(y)}{dy} \\end{equation*}\\] For example, suppose \\(X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\), then \\(Y=aX+b\\sim\\mathcal{N}(a\\mu+b, (a\\sigma)^2)\\). 2.4 Q4 Theorem 2.1 (Chebyshevâ€™s Inequality) Let \\(X\\) be a random variable with finite mean \\(\\mu\\) and variance \\(\\sigma^2&gt;0\\). Then \\(\\forall k&gt;0\\), \\[\\begin{equation*} P(|X-\\mu|\\geq k\\sigma) \\leq \\frac{1}{k^2} \\end{equation*}\\] Additionally, let \\(k\\sigma = \\varepsilon\\), the above becomes, \\[\\begin{equation*} P(|X-\\mu|\\geq \\varepsilon) \\leq \\frac{\\sigma^2}{\\varepsilon^2} \\end{equation*}\\] "],
["tutorial-3.html", "Chapter 3 Tutorial 3 3.1 Method of moment estimator(MME) 3.2 Maximum likelihood estimator(MLE).", " Chapter 3 Tutorial 3 3.1 Method of moment estimator(MME) Suppose that the problem is to estimate \\(k\\) unknown parameters \\(\\boldsymbol{\\theta} := (\\theta_1,\\theta_2,...,\\theta_k)^{T}\\) characterizing the distribution \\(f_X(x;\\boldsymbol{\\theta})\\) of the random variable \\(X\\). Suppose the first \\(k\\) moments of the true distribution can be expressed by the function of \\(\\boldsymbol{\\theta}\\), i.e., \\[\\begin{align} \\mu_{1} &amp; \\equiv \\mathrm{E}[W]=g_{1}\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k}\\right) \\\\ \\mu_{2} &amp; \\equiv \\mathrm{E}\\left[W^{2}\\right]=g_{2}\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k}\\right) \\\\ &amp; \\vdots \\\\ \\mu_{k} &amp; \\equiv \\mathrm{E}\\left[W^{k}\\right]=g_{k}\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k}\\right) \\end{align}\\] Suppose a sample of size \\(n\\) is drawn, having the values of \\(x_1,x_2,...,x_n\\), let \\[\\begin{align} \\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_i^j, j=1,2,...,k \\end{align}\\] Solve the above \\(k\\) equations, we derive the method of moment estimator of \\(\\boldsymbol{\\theta}\\). 3.2 Maximum likelihood estimator(MLE). Suppose we have a sample of size \\(n\\), \\(X_1,...,X_n\\) i.i.d drawn from a population distribution \\(f_X(x;\\boldsymbol{\\theta}), \\boldsymbol{\\theta} = (\\theta_1,\\theta_2,...,\\theta_k)^{T}\\). Define the likelihood function to be \\[ L(\\boldsymbol\\theta) = \\prod_{i=1}^n f(x_i;\\boldsymbol{\\theta}) \\] The log-likelihood function is defined by \\(\\ell(\\boldsymbol\\theta)=\\log L(\\boldsymbol\\theta)\\). The maximum likelihood estimator \\(\\hat{\\boldsymbol\\theta}\\) is determined to maximize \\(L(\\boldsymbol\\theta)\\), i.e., \\[\\begin{equation} \\hat{\\boldsymbol\\theta} =\\underset{\\boldsymbol\\theta\\in \\Theta}{\\max} L(\\boldsymbol\\theta) \\end{equation}\\] "],
["references.html", "References", " References "]
]
