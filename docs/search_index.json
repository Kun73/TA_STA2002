[
["index.html", "Tutorial for STA2002 Prerequisites", " Tutorial for STA2002 Kun HUANG(SDS) 2020-09-14 Prerequisites Probability and Statistics I(STA2001) is the prerequisite, which mainly includes the following contents, Some usual distributions, like Binomial, Poisson, Normal, Exponential, Gamma, and Chi-square distributions (Relationships among some univariate distributions(Song 2005)); Basic terminologies, e.g.,independence, expectation, variation, correlation (coefficient), Bayes, and etc; Large number theorem, like Central Limit Theorem(CLT). References "],
["sec-T1.html", "Chapter 1 Tutorial 1 1.1 Q1 1.2 Q2 1.3 Q3", " Chapter 1 Tutorial 1 1.1 Q1 Moment-generating function \\(M(t)\\) of a random variable \\(X\\) defined in \\(D\\) that has a density function \\(f(x)\\). \\[\\begin{align} M(t) = \\mathbb{E}(e^{tx}) &amp;= \\int_{D} e^{tx}f(x)dx\\\\ \\mathbb{E}(X^{s}) &amp;= M^{(s)}(0) \\end{align}\\] Relationship between \\(\\bar X=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\) and \\(S^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(X_i-\\bar X)^2\\), independent. How to derive a quantity following \\(t\\) distribution from a norm population. \\[\\begin{align} T=\\frac{\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) S^{2}}{\\sigma^{2}} /(n-1)}}=\\frac{\\bar{X}-\\mu}{S / \\sqrt{n}} \\end{align}\\] The \\(t\\) distribution is symmetric, i.e., \\(t_{q}(n) = -t_{1-q}(n), q\\in(0,1)\\). For example, qt(0.025, 8, lower.tail = F) ## [1] 2.306004 -qt(1 - 0.025, 8, lower.tail = F) ## [1] 2.306004 Properties of \\(F\\) distribution: \\(F_{0.95}(9,24)= \\frac{1}{F_{0.05}(24,9)}\\) 1.2 Q2 Standardize a norm distribution \\(X\\in\\mathcal{N}(\\mu,\\sigma^2)\\), i.e., \\(\\frac{X-\\mu}{\\sigma}\\in\\mathcal{N}(0,1)\\). The distribution of \\(\\bar X\\) and \\(S^2\\). 1.3 Q3 Central Limit Theorem(CLT) Theorem 1.1 (Central Limit Theorem) Let \\(X_1,\\ldots,X_n\\) be independent, identically distributed (i.i.d.) random variables with finite expectation \\(\\mu\\), and positive, finite variance \\(\\sigma^2\\), and set \\(S_n=X_1 + X_2 + \\cdots + X_n\\), \\(n \\ge 1\\). Then \\[ \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}\\xrightarrow{L} N(0, 1) ~\\mathrm{as}~n\\rightarrow\\infty. \\] The relationship between Binomial distribution \\(b(n,p)\\) and Poisson distribution \\(\\mathrm{Pois}(\\lambda)\\): \\(\\infty &gt; np = \\lambda, n\\rightarrow\\infty\\) Aware the power of CLT. "],
["sec-T2.html", "Chapter 2 Tutorial 2 2.1 Q1 2.2 Q2 2.3 Q3 2.4 Q4", " Chapter 2 Tutorial 2 2.1 Q1 Derive moments from a given pdf \\(f(x)\\). \\(EX = \\int xf(x)dx, EX^2=\\int x^2f(x)dx\\). Derive variance from the first and second moments,i.e., \\(Var(X)=EX^2-E^2X\\). \\(E(aX+bY+c) = aEX + bEY+c\\), \\(Var(aX+bY+c) = a^2Var(X)+b^2Var(Y)\\). The latter needs \\(X\\) and \\(Y\\) are independent. CLT approximation. 2.2 Q2 Definition 2.1 (Poisson Process) Let \\(N(t)\\) be the number of events happens during the time interval \\([0,t]\\), if \\(N(t)\\) satisfies the following: N(0) = 0; has independent increments, and \\(\\forall \\tau&gt;0\\), \\(P(N(t+\\tau)-N(t) = n)= \\frac{(\\lambda \\tau)^n}{n!}e^{-\\lambda \\tau}\\) we call \\(\\{N(t),t\\geq0\\}\\) is a Poisson process with rate \\(\\lambda\\). Let \\((W_n&gt;t)\\) be the \\(n-th\\) random event happens after time \\(t\\), then \\(W_n\\sim Gamma(n, \\lambda)\\). 2.3 Q3 Proposition 2.1 Suppose the random variable \\(X\\) has a pdf \\(f(x)\\), let \\(Y = T(X)\\), where \\(T:\\mathbb{R}\\rightarrow \\mathbb{R}\\) is an invertible transformation. Then the pdf \\(g(y)\\) of \\(Y\\) is \\[\\begin{equation*} g(y) = f(T^{-1}(y))\\frac{d T^{-1}(y)}{dy} \\end{equation*}\\] For example, suppose \\(X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\), then \\(Y=aX+b\\sim\\mathcal{N}(a\\mu+b, (a\\sigma)^2)\\). 2.4 Q4 Theorem 2.1 (Chebyshevâ€™s Inequality) Let \\(X\\) be a random variable with finite mean \\(\\mu\\) and variance \\(\\sigma^2&gt;0\\). Then \\(\\forall k&gt;0\\), \\[\\begin{equation*} P(|X-\\mu|\\geq k\\sigma) \\leq \\frac{1}{k^2} \\end{equation*}\\] "],
["references.html", "References", " References "]
]
